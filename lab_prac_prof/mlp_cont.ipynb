{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load data alreadytrain, and transform appply to data\n",
    "mnist_train = datasets.MNIST(\n",
    "    root=\"./mnist_data\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "#load test data no train , transforrm apply to data \n",
    "mnist_test = datasets.MNIST(\n",
    "    root=\"./mnist_data\", train=False, download=True, transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n",
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check size of data 60000 pic-train, 10000img-test\n",
    "print(len(mnist_train), len(mnist_test))\n",
    "\n",
    "#check shape of data\n",
    "print(mnist_train[0][0].shape, mnist_train[0][1]) #image size 1(greycale) *28*28 pixel\n",
    "#5 = first img [0][1] is digit show in imge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "batch_size = 32  # number of sample per batch Hyperparameter\n",
    "\n",
    "#load both data with the batchsize, random sample during each epoch\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#for test too but no random\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#see one batch from train loader just to check\n",
    "train_iter = iter(train_loader)\n",
    "imgs, labels = next(train_iter)\n",
    "\n",
    "#so one batch have 32 img shape 1*28*28\n",
    "print(imgs.shape, labels.shape) #labels shape is 32 per one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    #make layer (input, hidden, output)\n",
    "    def __init__(self, input_dimension: int, hidden_layer_sizes: List[int], output_dimension: int):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        previous_size = input_dimension\n",
    "        for hidden_size in hidden_layer_sizes:\n",
    "            self.hidden_layers.append(nn.Linear(previous_size, hidden_size))\n",
    "            previous_size = hidden_size\n",
    "        \n",
    "        self.output_linear = nn.Linear(previous_size, 10) #will have 10 output\n",
    "\n",
    "    #train use foword first \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, self.input_dimension) #same like reshape in np the input data into 1d. -1+2 = 1d\n",
    "        #print(x.shape)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.gelu(layer(x)) #send to hidden layer and use activation function gelu \n",
    "         #   print(x.shape)\n",
    "\n",
    "        z_3 = self.output_linear(x) # get output\n",
    "        #print(z_3.shape)\n",
    "        return z_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") # use cpu to train, can use gpu too for cuda\n",
    "\n",
    "# input use the image size which is 1*28*28 = 784\n",
    "input_dimension = 1*28*28\n",
    "output_dimension = 10 # get 10 output on output layer\n",
    "\n",
    "hidden_layer_size = [128,128,128] # hidden layer have 128 node each, have 3 layer\n",
    "num_epochs = 5 # iterate 5 times\n",
    "learning_rate = 0.01\n",
    "lr_decay_rate = 0.5 # decrese rate by 0.8 \n",
    "\n",
    "#start train \n",
    "model = MLPClassifier(input_dimension, hidden_layer_size, output_dimension)\n",
    "model = model.to(device)\n",
    "\n",
    "#get loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#use adam optimizer for learning rate to get good gradiernt decent\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#if learning rate to slow or will decrease use gamma by decay rate and optimizer for better training\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=lr_decay_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(imgs) # put img data into the model \n",
    "#print shape in foword x.shape\n",
    "#make 2d into 1d in foword\n",
    "\n",
    "#torch.Size([32, 1, 28, 28]) # raw data\n",
    "#torch.Size([32, 784]) 28*28= 784 first hidden like 784,128- input layer\n",
    "#torch.Size([32, 128]) = next hidden layer 1hidden layer\n",
    "#torch.Size([32, 128]) =next hidden l - 2nd hidden layer\n",
    "#torch.Size([32, 128]) = same - 3rd hidden layer\n",
    "#torch.Size([32, 10]) = last output should be 32,10 - output layer - 10 output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1067, 0.0963, 0.0901, 0.1055, 0.0962, 0.1008, 0.0952, 0.1009, 0.1038,\n",
       "        0.1044], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# convert result raw score into probabilities sum to 1, for 10 output when add all will =1 \n",
    "# highest prob more accurate\n",
    "torch.softmax(output[0], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# next calc for backward before traning\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "\n",
    "    loss = 0.0 # calc loss\n",
    "    accuracy = 0.0 # calc accuracy\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_images, labels in test_loader:\n",
    "            input_images = input_images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #print(input_images.shape) # input raw 32 1 28 28\n",
    "            #print(labels.shape) # label shape the batch size 32\n",
    "\n",
    "            outputs = model(input_images)\n",
    "            #print(outputs.shape) # outut shape 32 10\n",
    "            #(batch_size, num_classes)\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            #print(predicted) # predict a lot 0 since before training\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            total += batch_size\n",
    "            \n",
    "            #print((predicted==labels)) # check the prediction true or false\n",
    "            #print((predicted==labels).sum()) # check total true =  5\n",
    "\n",
    "            accuracy += (predicted == labels).sum().item() # calc accuray based on the predict n label\n",
    "            loss += criterion(outputs, labels).item() * batch_size # calc loss\n",
    "\n",
    "            #print(criterion(outputs, labels).item()) # check loss \n",
    "            #break\n",
    "\n",
    "    accuracy /= total\n",
    "    loss /= total\n",
    "\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0884, 2.305179496765137)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# run the evaluate with model  loader and data , return calc or loss and accuracy\n",
    "evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "#torch.Size([32, 1, 28, 28])- input\n",
    "#torch.Size([32])\n",
    "#torch.Size([32, 1, 28, 28])\n",
    "#torch.Size([32, 784])\n",
    "#torch.Size([32, 128])\n",
    "#torch.Size([32, 128])\n",
    "#torch.Size([32, 128])\n",
    "#torch.Size([32, 10])\n",
    "#torch.Size([32, 10])\n",
    "#tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#        0, 0, 0, 0, 0, 0, 0, 0])\n",
    "#tensor([False, False, False,  True, False, False, False, False, False, False,\n",
    "#         True, False, False,  True, False, False, False, False, False, False,\n",
    "#       False, False, False, False, False,  True, False, False,  True, False,\n",
    "##        False, False])\n",
    "#tensor(5)\n",
    "#2.3018720149993896 # total loss since before training\n",
    "# accuracy , loss\n",
    "#(0.15625, 2.3018720149993896)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test initial - accuracy: 0.0884  loss: 2.3052\n",
      "Train epoch - 0/ 5 - accuracy: 0.9143  loss: 0.3198\n",
      "Test epoch - 0/ 5 - accuracy: 0.9410  loss: 0.2273\n",
      "Train epoch - 1/ 5 - accuracy: 0.9622  loss: 0.1363\n",
      "Test epoch - 1/ 5 - accuracy: 0.9648  loss: 0.1298\n",
      "Train epoch - 2/ 5 - accuracy: 0.9766  loss: 0.0805\n",
      "Test epoch - 2/ 5 - accuracy: 0.9702  loss: 0.1135\n",
      "Train epoch - 3/ 5 - accuracy: 0.9846  loss: 0.0523\n",
      "Test epoch - 3/ 5 - accuracy: 0.9774  loss: 0.0906\n",
      "Train epoch - 4/ 5 - accuracy: 0.9899  loss: 0.0340\n",
      "Test epoch - 4/ 5 - accuracy: 0.9789  loss: 0.0899\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# to test the accury and loss. evaluate the accuracy and loss\n",
    "eval_accuracy, eval_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test initial - accuracy: {eval_accuracy:.4f}  loss: {eval_loss:.4f}\") #from eval\n",
    "\n",
    "#train for each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model.train() # train the model loop to epoch num = 5 the one we declare above\n",
    "    for inputs, labels in train_loader: # loop over each batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs) # foword pass\n",
    "        loss = criterion(outputs, labels) # compute loss\n",
    "        loss.backward() # compute gradient\n",
    "\n",
    "        #gradient desent update step with alpha val\n",
    "        optimizer.step() # update model weight use the gradient in backward we got\n",
    "        optimizer.zero_grad() # clear gradient for next epoch\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        train_loss += loss.item()* batch_size # calc total lost\n",
    "\n",
    "        predicted = torch.argmax(outputs, dim=1) # predict class\n",
    "        train_accuracy += (predicted == labels).sum().item() # calc total accuracy\n",
    "\n",
    "    lr_scheduler.step() # update learning rate after each epoch\n",
    "\n",
    "    train_accuracy /= len(train_loader.dataset) #make mean with accuracy\n",
    "    train_loss /= len(train_loader.dataset) # make mean with loss\n",
    "\n",
    "    model.eval() # eval model, disable dropout\n",
    "    eval_accuracy, eval_loss = evaluate(model,test_loader, criterion, device) # eval use data\n",
    "\n",
    "    print(f\"Train epoch - {epoch}/ {num_epochs} - accuracy: {train_accuracy:.4f}  loss: {train_loss:.4f}\")\n",
    "    print(f\"Test epoch - {epoch}/ {num_epochs} - accuracy: {eval_accuracy:.4f}  loss: {eval_loss:.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test epoch - 4/ 5 - accuracy: 0.9688  loss: 0.1788 - 0.02 ,0.8\n",
    "#-0.01,0.7\n",
    "\n",
    "#Test initial - accuracy: 0.1562  loss: 2.2850\n",
    "#Train epoch - 0/ 5 - accuracy: 0.8824  loss: 0.4533\n",
    "#Test epoch - 0/ 5 - accuracy: 0.9375  loss: 0.2239\n",
    "#Train epoch - 1/ 5 - accuracy: 0.9392  loss: 0.2273\n",
    "#Test epoch - 1/ 5 - accuracy: 1.0000  loss: 0.0639\n",
    "#Train epoch - 2/ 5 - accuracy: 0.9580  loss: 0.1554\n",
    "#Test epoch - 2/ 5 - accuracy: 0.9688  loss: 0.0591\n",
    "#Train epoch - 3/ 5 - accuracy: 0.9681  loss: 0.1103\n",
    "#Test epoch - 3/ 5 - accuracy: 0.9688  loss: 0.0454\n",
    "#Train epoch - 4/ 5 - accuracy: 0.9745  loss: 0.0856\n",
    "#Test epoch - 4/ 5 - accuracy: 1.0000  loss: 0.0116\n",
    "#Test epoch - 4/ 5 - accuracy: 1.0000  loss: 0.0116 - 0.02, 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
