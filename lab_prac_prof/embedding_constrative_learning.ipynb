{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news.json\") as f:\n",
    "    news_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9604/9604 [00:02<00:00, 3901.78it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "invalid_line_patterns = re.compile(\n",
    "    r\"무단\\s*전재|배포\\s*금지|Copyrights|관련기사|기사\\s*제보|여러분의 제보|카카오톡\\s*:s*\"\n",
    ")\n",
    "\n",
    "for article in tqdm(news_list, mininterval=1):\n",
    "    text = article[\"text\"]\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    filtered_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = \" \".join(line.split())\n",
    "\n",
    "        # OOO 기자 패턴 제거\n",
    "        line = re.sub(r\"\\w+ 기자\", \"\", line)\n",
    "        # email 주소 제거\n",
    "        line = re.sub(r\"\\w+@\\w+\\.\\w+\", \"\", line)\n",
    "        # 뉴스에서 []는 [논산] [대구=뉴시스] 등 다양한 참조를 의미하므로 제거\n",
    "        line = re.sub(r\"\\[.*\\]\", \"\", line)\n",
    "        # (서울=연합뉴스) 같은 패턴 제거\n",
    "        line = re.sub(r\"\\(.*=.*\\)\", \"\", line)\n",
    "        # 2024.10.29/뉴스1 같은 패턴 제거\n",
    "        line = re.sub(r\"\\d{4}\\.\\d{2}\\.\\d{2}/.*\\b\", \"\", line)\n",
    "\n",
    "        # 공백 제거\n",
    "        line = \" \".join(line.split())\n",
    "\n",
    "        if invalid_line_patterns.search(line):\n",
    "            # 무단전재, 배포금지 등이 포함된 문장 이후 문장들은 제외\n",
    "            break\n",
    "\n",
    "        # 한국어가 10자 이상 포함된 경우만 포함\n",
    "        num_korean_chars = len(re.findall(r\"[ㄱ-ㅎ가-힣]\", line))\n",
    "        if num_korean_chars >= 10:\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    text = \"\\n\".join(filtered_lines)\n",
    "    # 소문자로 변환\n",
    "    text = text.lower()\n",
    "\n",
    "    # 한국어가 50자 이상 포함된 경우만 포함\n",
    "    num_korean_chars = len(re.findall(r\"[ㄱ-ㅎ가-힣]\", text))\n",
    "    if num_korean_chars >= 50:\n",
    "        documents.append(text)\n",
    "\n",
    "# 중복 제거\n",
    "documents = list(set(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7880/7880 [02:22<00:00, 55.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi() #load kiwi\n",
    "\n",
    "document_tokens_list = []\n",
    "\n",
    "for document in tqdm(documents, mininterval=1):\n",
    "    #tokenize use kiwi\n",
    "    tokens = [token.form for token in kiwi.tokenize(document)]\n",
    "\n",
    "    #if token >= 10 put in doct list\n",
    "    if len(tokens) >= 10:\n",
    "        document_tokens_list.append((document, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "dimension = 128 #convert word into 128 dimensional vector\n",
    "#similar word = similar vector\n",
    "#unsame word = far apart (negative words)\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    #use clean data from token to train\n",
    "    sentences=[tokens for _, tokens in document_tokens_list],\n",
    "    vector_size=dimension,\n",
    "    min_count=10, #ignore word come less than 10. rare word are noise\n",
    "    sg=1, # sg=1 = skipgram sg= 0 cbow\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13351"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.wv.key_to_index) #see the word that got embedded. word:index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14436108, -0.6728425 ,  0.42717922, -0.02503245,  0.28129384,\n",
       "       -0.5748861 , -0.13230786, -0.26044425,  0.28304553, -0.1035008 ,\n",
       "        0.02134698, -0.25134763, -0.6525757 ,  0.29241303,  0.30718377,\n",
       "        0.3168591 , -0.2690622 ,  0.6378406 ,  0.20351551,  0.13090998,\n",
       "        0.47144797,  0.23624171,  0.15637122, -0.22894427,  0.34819803,\n",
       "        0.53402597,  0.29751694, -0.07736859,  0.44035646,  0.04821702,\n",
       "        0.12179998,  0.81935346,  0.0901124 ,  0.06763581, -0.32814705,\n",
       "        0.7698681 ,  0.5220625 , -0.22950764, -0.09775116,  0.35656187,\n",
       "        0.49254754,  0.38233575, -0.15546325, -0.18273942,  0.07226826,\n",
       "        0.55556023, -0.10163607, -0.36923897, -0.7952047 , -0.06844834,\n",
       "        0.10075079, -0.37676796,  0.13768177, -0.01207493, -0.06462228,\n",
       "        0.17607294,  0.64535826, -0.5870449 , -0.5829941 ,  0.03188203,\n",
       "        0.26531914,  0.07526039,  0.18578029, -0.25034085,  1.1055132 ,\n",
       "        0.4380595 , -0.51028204,  0.04706103,  0.48990476, -0.13018319,\n",
       "       -0.47097522, -0.35379618, -0.09029834,  0.43032226,  0.05957955,\n",
       "       -0.37431666,  0.16500837, -0.20203643,  0.6113316 , -0.37684634,\n",
       "       -0.278247  , -0.5959607 , -0.26973447,  0.09583132,  0.7351091 ,\n",
       "       -0.11705927,  0.31537303, -0.15628462,  0.32874677,  0.27071118,\n",
       "        0.9300177 , -0.0929143 , -0.09155269,  0.04310733,  0.51528597,\n",
       "       -0.05230824, -0.4372697 , -0.44063607,  0.03528901, -0.5236145 ,\n",
       "        0.05187876, -0.0603848 , -0.5366294 ,  0.41061246, -0.20337012,\n",
       "        0.35649332, -0.01332538, -0.23530936,  0.04256951, -0.15445425,\n",
       "       -0.12727062, -0.23361395, -0.02409306,  0.230078  ,  0.11449513,\n",
       "       -0.15839936, -0.18116187,  0.44610822,  0.04765944, -0.13406362,\n",
       "       -0.44508302, -0.260469  , -0.14215808,  0.03843389, -0.6429535 ,\n",
       "       -0.86614203,  0.03579983, -0.45023295], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv[\"저출산\"]  #show in 128 dimenstio matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('우크라이나', 0.7159690260887146),\n",
       " ('벨라루스', 0.6924394965171814),\n",
       " ('푸틴', 0.6633460521697998),\n",
       " ('모스크바', 0.662726879119873),\n",
       " ('선희', 0.6476637721061707),\n",
       " ('오베르추크', 0.6438256502151489),\n",
       " ('최선희', 0.6383201479911804),\n",
       " ('침공', 0.6325005888938904),\n",
       " ('알렉세이', 0.6201255917549133),\n",
       " ('외무상', 0.6175360083580017)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_word(\"저출산\", 10) #print 10 similar by word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6451614"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similarity(\"남자\", \"여자\") #check word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64516145\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w1 = word2vec.wv[\"남자\"]\n",
    "w2 = word2vec.wv[\"여자\"]\n",
    "\n",
    "print(np.dot(w1, w2) / (np.linalg.norm(w1) * np.linalg.norm(w2)))#check word similarity \n",
    "#but use manual log calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('한껏', 0.36258846521377563),\n",
       " ('걸맞', 0.34917300939559937),\n",
       " ('번창', 0.33775898814201355),\n",
       " ('국력', 0.3361835777759552),\n",
       " ('튼튼', 0.33169025182724),\n",
       " ('일류', 0.3180806338787079),\n",
       " ('우리', 0.31189751625061035),\n",
       " ('한류', 0.3095971345901489),\n",
       " ('우리나라', 0.3093096613883972),\n",
       " ('국운', 0.3077029287815094),\n",
       " ('저력', 0.3011007010936737),\n",
       " ('국격', 0.29771944880485535),\n",
       " ('절호', 0.2946532368659973),\n",
       " ('군대', 0.2889138460159302),\n",
       " ('국익', 0.28568434715270996),\n",
       " ('매개체', 0.2835228145122528),\n",
       " ('보유국', 0.28136005997657776),\n",
       " ('라며', 0.27619534730911255),\n",
       " ('눈부시', 0.27489134669303894),\n",
       " ('지평', 0.27459338307380676)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive=[\"서울\", \"수도권\"], negative=[\"지방\"], topn=20)\n",
    "#get similar  20 word to positive avoid negative word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('확실히', 0.1707804948091507),\n",
       " ('성과', 0.15819254517555237),\n",
       " ('기회', 0.1513046771287918),\n",
       " ('튼튼', 0.1455494612455368),\n",
       " ('우선순위', 0.13711942732334137),\n",
       " ('자주국방', 0.13555872440338135),\n",
       " ('성취', 0.13308386504650116),\n",
       " ('매개체', 0.13031631708145142),\n",
       " ('세심', 0.12953375279903412),\n",
       " ('치밀', 0.12358049303293228),\n",
       " ('예우', 0.12189313769340515),\n",
       " ('결과물', 0.11718796193599701),\n",
       " ('겠', 0.11632491648197174),\n",
       " ('국익', 0.11589697748422623),\n",
       " ('튼튼하', 0.11494424194097519),\n",
       " ('응원', 0.11228194832801819),\n",
       " ('ᆸ시다', 0.1107255220413208),\n",
       " ('걸맞', 0.11027456074953079),\n",
       " ('고개', 0.1087050661444664),\n",
       " ('부합', 0.10795063525438309)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive=[\"지방\"], negative=[\"서울\", \"수도권\"], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "dimension = 128 #dimen\n",
    "\n",
    "fasttext = FastText(\n",
    "    sentences=[tokens for _, tokens in document_tokens_list],\n",
    "    vector_size=dimension,\n",
    "    min_count=10, #< 10 ignore\n",
    "    sg=1, #skipgram\n",
    "    workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('마포', 0.5779337286949158),\n",
       " ('포 시즌스', 0.5515439510345459),\n",
       " ('영등포구', 0.5414703488349915),\n",
       " ('서초동', 0.5329918265342712),\n",
       " ('영등포', 0.5272544026374817),\n",
       " ('부천', 0.5096818208694458),\n",
       " ('강북', 0.5036703944206238),\n",
       " ('성수동', 0.5022822022438049),\n",
       " ('삼성동', 0.501463770866394),\n",
       " ('마포구', 0.4974789321422577),\n",
       " ('서울역', 0.4964357018470764),\n",
       " ('둔촌동', 0.49513500928878784),\n",
       " ('아파트값', 0.4916444420814514),\n",
       " ('역삼동', 0.4903755486011505),\n",
       " ('롯데월드타워', 0.48988643288612366),\n",
       " ('한남동', 0.4877351224422455),\n",
       " ('노원구', 0.48681965470314026),\n",
       " ('시청역', 0.4854176938533783),\n",
       " ('페어몬트', 0.4853866994380951),\n",
       " ('청량리', 0.4812600612640381)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext.wv.most_similar(positive=[\"서울\", \"수도권\"], negative=[\"지방\"], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "dimension = 128\n",
    "\n",
    "#train doc2vec - document embedding , larn one vector for 1 doc, 1 vector per doc\n",
    "doc2vec = Doc2Vec(\n",
    "    documents=[\n",
    "        #create tag for each doc\n",
    "        TaggedDocument(words=tokens, tags=[str(i)])\n",
    "        for i, (_, tokens) in enumerate(document_tokens_list)\n",
    "    ],\n",
    "    vector_size=dimension,\n",
    "    min_count=10,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('전셋값', 0.6177747845649719),\n",
       " ('송파구', 0.5656211376190186),\n",
       " ('노원구', 0.5509284734725952),\n",
       " ('성북', 0.525770366191864),\n",
       " ('성동구', 0.5174793004989624),\n",
       " ('땅값', 0.5143045783042908),\n",
       " ('동대문구', 0.510901153087616),\n",
       " ('938', 0.49799153208732605),\n",
       " ('성수동', 0.49796727299690247),\n",
       " ('중구', 0.494922935962677)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.wv.most_similar(positive=[\"강남구\", \"집값\"])\n",
    "#get similar word with positive word but in doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02801296, -0.19658181, -0.07889026,  0.01073649, -0.01298841,\n",
       "        0.09010877,  0.03856221, -0.02263764, -0.08337904,  0.00778542,\n",
       "        0.05301411,  0.02931999, -0.11233705, -0.00940534,  0.02868988,\n",
       "       -0.05450509,  0.02084727, -0.01695624,  0.11336904,  0.06300487,\n",
       "        0.01923266,  0.02674555, -0.08762868, -0.04431376,  0.09776855,\n",
       "       -0.0275414 , -0.10705452, -0.02760827,  0.03714631, -0.1399316 ,\n",
       "        0.0597698 ,  0.04151881,  0.0027904 , -0.0066614 , -0.00566008,\n",
       "       -0.03267984,  0.08474678, -0.15980522, -0.0524998 ,  0.09186453,\n",
       "       -0.04463796,  0.0184387 , -0.01826465,  0.03412673,  0.09043032,\n",
       "       -0.05506077,  0.05100982, -0.0872625 , -0.07648489, -0.04177199,\n",
       "       -0.01950743, -0.02489886,  0.00837776, -0.05121358, -0.06435177,\n",
       "       -0.09231475,  0.09234323, -0.01025608, -0.03503404, -0.07719912,\n",
       "       -0.10302018, -0.00104833, -0.02459497, -0.05337315,  0.01846555,\n",
       "       -0.08057377,  0.09168082, -0.0095262 , -0.01199139,  0.00560964,\n",
       "        0.00930238,  0.17196941,  0.06733573,  0.01656497, -0.01822138,\n",
       "       -0.02251964, -0.02198993,  0.11465652, -0.03290276,  0.04873831,\n",
       "        0.1306345 , -0.01804635,  0.03757505, -0.00369186,  0.07072252,\n",
       "       -0.09684632, -0.02151673, -0.01073674,  0.10680649,  0.07161564,\n",
       "        0.04186649,  0.00166439, -0.05043645,  0.09084191,  0.09475411,\n",
       "        0.12413975,  0.03024393,  0.01023583, -0.05321315,  0.09637916,\n",
       "       -0.12046281,  0.048187  , -0.0071208 ,  0.02394333,  0.02597655,\n",
       "       -0.06241073, -0.1164303 , -0.07189711, -0.01923005, -0.07039465,\n",
       "        0.13056424,  0.05687816,  0.02131632,  0.01058799,  0.03766794,\n",
       "        0.02908227,  0.15542457, -0.04392087,  0.08359798, -0.07755341,\n",
       "       -0.14447914, -0.05807455, -0.00442715,  0.09406698,  0.0640071 ,\n",
       "        0.03475188,  0.0359289 , -0.00716967], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.wv.get_mean_vector([\"강남구\", \"집값\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“최근 아파트 매매 거래량이 증가하고 있고 서울 강남 3구(강남·서초·송파)의 집값이 전고점의 95% 수준까지 올라오는 등 회복기에 들어선 것으로 판단됩니다. 서울의 경우 1990년대생들이 주목하는 뉴타운 재개발 단지를 중심으로 집값이 더 상승할 것으로 예상합니다. 다만 교통과 일자리, 학군 등에 따른 집값 양극화 현상은 점점 더 심화할 것으로 보여 상급지\n",
      "\n",
      "윤석열 정부가 8·8 부동산 대책을 내놨다. 폭등하는 서울과 수도권 아파트 값·전세값을 잡기 위한 대책이다. 서울 주변 그린벨트를 풀어 주택 공급을 확대하고, 스트레스 dsr(총부채원리금상환비율) 금리 상향으로 수요를 억제함으로써 서울 집값을 내리게 하겠다는 게 주요 골자다. 대책이 나온 지도 한달 보름이 훌쩍 넘었다. 그럼에도 서울과 서울 인근 집값은 계\n",
      "\n",
      "정부의 오락가락 정책에 하반기 집값 향뱡을 두고 전문가들의 전망이 엇갈리고 있다. 9월부터 스트레스 dsr(총부채원리금상환비율) 2단계가 시행되면 대출이 어려워져 집값이 잡힐 것이란 예상이 있다. 반면 하반기 금리 인하가 시작하면 집값이 본격 상승할 것이란 이들도 많다.\n",
      "27일 부동산업계에 따르면 지난해 말부터 서울 아파트값 상승세가 계속되면서 서울 아파트\n",
      "\n",
      "주택 가액 기준이 9억원 이하 제한\n",
      "신생아 특례대출 신청이 출시 5개월 만에 5조8597억원의 대출 신청이 들어온 것으로 확인됐다. 서울보다는 경기, 인천의 대출 신청 건이 많았다. 가계 대출 확대를 부추기고 집값 상승을 자극할 수 있다는 우려가 나온다.\n",
      "신생아 특례대출은 대출 신청일 기준으로 2년 이내에 출산·입양한 무주택 가구나 1주택 가구(대환대출)에\n",
      "\n",
      "정부가 집값 불안을 잠재우기 위해 지난 8월 8일 정부서울청사에서 ‘제8차 부동산 관계 장관회의’를 열고 ‘국민 주거 안정을 위한 주택공급 확대 방안(8·8 주택공급 대책)’을 발표하며 총력전에 나섰지만, 8월 둘째 주 서울 아파트 가격은 5년 11개월 만에 최대 폭인 0.32%나 급격히 올라가는 등 부동산 시장의 움직임은 여전히 심상치 않다. 가계대출 증\n",
      "\n",
      "최근 kb부동산에 따르면 서울 아파트 평균 매매가격은 지난달 기준 12억1387만원으로 나타났다. 2022년 금리가 급증한 이후 계속 조정됐던 서울 집값은 지난해 5월 11억8404만원까지 내려가기도 했지만, 다시 반등하기 시작해 올해 들어서 다시 12억원을 넘어선 것이다.\n",
      "단적인 예로 2024 파리올림픽 3관왕에 오른 양궁 김우진이 10억원이 넘는 포상금\n",
      "\n",
      "리얼캐스트가 핀테크 기업 핀다의 ai 상권 분석 플랫폼 ‘오픈업’을 통해 서울시 입시 학원 매출 데이터를 분석했습니다. 그 결과, 강남3구와 목동 비중이 대다수를 차지한 것으로 나타났습니다.\n",
      "핀다 오픈업의 ai 추정 매출을 토대로 서울시 법정동 467곳을 살펴봤습니다. 오픈업은 국내 대형 카드사와 통신사를 비롯해 국토부와 국세청, 통계청, 행정안전부 등 다\n",
      "\n",
      "［월요신문=］인천시가 저출생 육아지원 정책을 잇따라 내놓으면서 서울과 경기에서 이사 오는 수요를 더욱 촉발할지 주목받고 있다.\n",
      "18일 업계에 따르면 인천시는 아이를 낳는 인천시민에게 1억원을 지원하는 사업에 이어 신혼(예비)부부에게 하루 1000원꼴인 임대료의 주택을 빌려주는 저출생 정책을 마련, 내년부터 시행하겠다고 밝혔다.\n",
      "이른바 '천원주택'은 매입임대\n",
      "\n",
      "“8·8대책 이후 상승세 둔화하고 있는 건 사실”\n",
      "“신생아 특례 등 정책대출이 집값 상승 원인 아냐”\n",
      "박상우 국토교통부 장관이 8·8 공급대책 발표 이후 집값 오름세가 지속되고 있지만 상승폭은 둔화되고 있다고 평가했다. 아울러 신생아특례대출 등 정책대출이 집값 상승의 직접적인 원인이 아니라는 분석도 내놓았다.\n",
      "박 장관은 9일 정부세종청사에서 간담회에서 최근\n",
      "\n",
      "서울 진출입 교통 인프라 확충\n",
      "과세 정책 개편 수요 분산 유도\n",
      "집값 상승 근본원인 해결 시급\n",
      "서울 주택가격이 오름세를 보이면서 수도권 주택가격도 들썩이고 있다. 주택가격 상승은 가계부채를 늘려 한국은행의 금리 인하를 지연시킬 뿐만 아니라 주거비용을 높여 임금을 인상시키고 부의 불평등을 심화한다는 점에서 우려하지 않을 수 없다. 그동안 정책당국은 다양한 부동\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_indices = doc2vec.dv.similar_by_vector(\n",
    "    doc2vec.wv.get_mean_vector([\"강남구\", \"집값\"])\n",
    ")\n",
    "\n",
    "#find similar doc to mean vector doc\n",
    "for doc_index, similarity in doc_indices:\n",
    "    #print doc to that index\n",
    "    print(document_tokens_list[int(doc_index)][0][:200])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dv1 = doc2vec.wv.get_mean_vector([\"강남구\", \"집값\"])\n",
    "#get mean vector for the word\n",
    "\n",
    "#get mean by manual\n",
    "dv2 = np.mean(\n",
    "    [doc2vec.wv.get_vector(token, norm=True) for token in [\"강남구\", \"집값\"]],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "#compare = true since mean same\n",
    "print(np.allclose(dv1, dv2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
